{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f5eb82-bfb8-4995-8f07-da13e95abb3c",
   "metadata": {},
   "source": [
    "# Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0967cfca-7035-42b7-bd7d-1454150333a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_frames_from_video(video_path, frame_interval=15):\n",
    "    \"\"\"\n",
    "    Extract frames from a single video and return them as a list for later processing.\n",
    "    \n",
    "    Args:\n",
    "    - video_path (str): Path to the input video file.\n",
    "    - frame_interval (int): Interval for frame extraction (e.g., extract every 30th frame).\n",
    "    \n",
    "    Returns:\n",
    "    - frames (list): A list of frames extracted from the video.\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total frame count and frame rate\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"Processing {video_path} - Total frames: {total_frames}, FPS: {fps}\")\n",
    "\n",
    "    frames = []\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract the frame every `frame_interval` frames\n",
    "        if count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "\n",
    "    print(f\"Extracted {len(frames)} frames from {video_path}\")\n",
    "    \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3d575-05f1-4f00-9ebc-c71c8a21a6bb",
   "metadata": {},
   "source": [
    "# Faces Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024c2031-3fc9-4767-88bd-a2503086d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "def extract_highest_confidence_face_from_frames(frames):\n",
    "    \"\"\"\n",
    "    Extract the face with the highest confidence score from a list of frames using MTCNN.\n",
    "    :param frames: List of frames (images) extracted from the video.\n",
    "    :return: List of the highest confidence face images from each frame.\n",
    "    \"\"\"\n",
    "    mtcnn = MTCNN()\n",
    "\n",
    "    face_images = []  \n",
    "\n",
    "    for idx, frame in enumerate(frames):\n",
    "        faces = mtcnn.detect_faces(frame)\n",
    "\n",
    "        if faces:\n",
    "            highest_confidence_face = None\n",
    "            max_confidence = 0  # Variable to store the highest confidence\n",
    "\n",
    "            for face in faces:\n",
    "                confidence = face['confidence']\n",
    "                if confidence > max_confidence:\n",
    "                    max_confidence = confidence\n",
    "                    highest_confidence_face = face\n",
    "\n",
    "            if highest_confidence_face:\n",
    "                x, y, w, h = highest_confidence_face['box']\n",
    "                face_crop = frame[y:y+h, x:x+w]\n",
    "                face_images.append(face_crop)\n",
    "\n",
    "    print(f\"Extracted {len(face_images)} faces with the highest confidence.\")\n",
    "    return face_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57692c-8c2c-41c3-833c-d0ce276cd751",
   "metadata": {},
   "source": [
    "# Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0160e9a-0669-4a97-8211-ef0e8876db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546625ce-077b-4810-bdee-7498cdb38b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (224, 224))\n",
    "    image_preprocessed = preprocess_input(image_resized) \n",
    "    image_batch = np.expand_dims(image_preprocessed, axis=0)\n",
    "\n",
    "    return image_batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf74fe9e-f193-4cdf-a508-7a7f9a2c92f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(faces):\n",
    "    features = []\n",
    "\n",
    "    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')  \n",
    "    feature_extractor = Model(inputs=model.input, outputs=model.output)  \n",
    "    \n",
    "    for face in faces:\n",
    "        face_n = preprocess_image(face)  \n",
    "        face_feature = feature_extractor.predict(face_n) \n",
    "        features.append(face_feature)  \n",
    "    \n",
    "    features = np.concatenate(features, axis=0)   \n",
    "    features = np.expand_dims(features, axis=0)  \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46e700-6af2-4fcc-801e-8ec0132d338e",
   "metadata": {},
   "source": [
    "# DeepFakes Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a59549a-a027-448d-8b32-d0c4d5928437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfakes_detection(path):\n",
    "    frames = extract_frames_from_video(path)\n",
    "    faces = extract_highest_confidence_face_from_frames(frames)\n",
    "    features =  feature_extraction(faces)\n",
    "    x = pad_sequences(features, maxlen=108, dtype='float32', padding='post', truncating='post')\n",
    "    model = load_model(\"my_model.keras\")\n",
    "    model.compile(optimizer='rmsprop', loss='your_loss_function', metrics=['accuracy'])\n",
    "    y = model.predict(np.array(x))\n",
    "    print(y)\n",
    "    y = (y> 0.5).astype(int)\n",
    "\n",
    "    if y[0] == 0 : print(\"Real\")\n",
    "    else: print(\"Fake\")\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31f04727-d0ef-4b94-9aba-2490d6876510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\abder\\Downloads\\I_wish_I_could..._2019.mp4 - Total frames: 579, FPS: 29.97002997002997\n",
      "Extracted 39 frames from C:\\Users\\abder\\Downloads\\I_wish_I_could..._2019.mp4\n",
      "Extracted 39 faces with the highest confidence.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abder\\anaconda3\\anaconda3-1\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 9 variables whereas the saved optimizer has 16 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594ms/step\n",
      "[[0.99984455]]\n",
      "Fake\n"
     ]
    }
   ],
   "source": [
    "y = deepfakes_detection(r\"C:\\Users\\abder\\Downloads\\I_wish_I_could..._2019.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
